================================================================
RepopackPy Output File
================================================================

This file was generated by RepopackPy on: 2025-03-30T21:52:44.096805

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and RepopackPy's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

For more information about RepopackPy, visit: https://github.com/abinthomasonline/repopack-py

================================================================
Repository Structure
================================================================
README.md
configs_stock.py
ddpg.py
env.py
evaluator.py
main.py
memory.py
model.py
random_process.py
requirements.txt
util.py

================================================================
Repository Files
================================================================

================
File: random_process.py
================
import numpy as np

class RandomProcess(object):
    def reset_states(self):
        pass

class AnnealedGaussianProcess(RandomProcess):
    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):
        self.mu = mu
        self.sigma = sigma
        self.n_steps = 0

        if sigma_min is not None:
            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)
            self.c = sigma
            self.sigma_min = sigma_min
        else:
            self.m = 0.0
            self.c = sigma
            self.sigma_min = sigma

    @property
    def current_sigma(self):
        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)
        return sigma

# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab
class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):
    def __init__(
        self,
        theta,
        mu=0.0,
        sigma=1.0,
        dt=1e-2,
        x0=None,
        size=1,
        sigma_min=None,
        n_steps_annealing=1000,
    ):
        super(OrnsteinUhlenbeckProcess, self).__init__(
            mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing
        )
        self.theta = theta
        self.mu = mu
        self.dt = dt
        self.x0 = x0
        self.size = size
        self.reset_states()

    def sample(self):
        x = (
            self.x_prev
            + self.theta * (self.mu - self.x_prev) * self.dt
            + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)
        )
        self.x_prev = x
        self.n_steps += 1
        return x

    def reset_states(self):
        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)

================
File: env.py
================
from configs_stock import *
from empyrical import sharpe_ratio, max_drawdown, calmar_ratio, sortino_ratio
import pyfolio
import pickle
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from gym.utils import seeding
import gym
from gym import spaces
import matplotlib
import os

matplotlib.use("Agg")

class StockEnvTrade(gym.Env):
    """A stock trading environment for OpenAI gym"""

    metadata = {"render.modes": ["human"]}

    def __init__(
        self,
        all_data,
        day,
        args,
        turbulence_threshold=140,
        initial=True,
        previous_state=[],
        model_name="",
        iteration="",
    ):
        """
        all_data: list containing the dataset.
        day: the date on which the agent will start trading.
        last_day: last_day in the dataset.
        """

        self.args = args
        self.day = day
        self.all_data = all_data
        self.data = self.all_data[self.day]
        self.initial = initial
        self.previous_state = previous_state

        # action_space normalization and shape is STOCK_DIM
        self.action_space = spaces.Box(low=-1, high=1, shape=(STOCK_DIM,))
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(FEAT_DIMS,)
        )
        self.terminal = False
        self.turbulence_threshold = turbulence_threshold

        # initalize state
        last_price = self.data["adj_close_last"].view(-1).tolist()
        target_price = self.data["adj_close_target"].view(-1).tolist()
        len_data = self.data["length_data"].view(-1).tolist()
        emb_data = self.data["embedding"].view(-1).tolist()
        text_diff = self.data["text_difficulty"].view(-1).tolist()
        vol_diff = self.data["volatility"].view(-1).tolist()
        price_text_diff = self.data["price_text_difficulty"].view(-1).tolist()
        price_diff = self.data["price_difficulty"].view(-1).tolist()
        all_diff = self.data["price_text_vol_difficulty"].view(-1).tolist()
        time_feats = self.data["time_features"].view(-1).tolist()
        self.state = (
            [INITIAL_ACCOUNT_BALANCE]  # balance
            + last_price  # stock prices initial
            + [0] * STOCK_DIM  # stocks on hold
            + emb_data  # tweet features
            + len_data  # tweet len
            + target_price  # target price
            + price_diff
            + vol_diff
            + text_diff
            + price_text_diff
            + all_diff
            + time_feats
        )
        # initialize reward
        self.reward = 0
        self.turbulence = 0
        self.cost = 0
        self.trades = 0
        # memorize all the total balance change
        # value of assets cash + shares
        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]
        # biased CL rewards, during test all diff values are 1 so no change will happen in rewards
        self.rewards_memory = []
        # self.reset()
        self._seed()
        self.model_name = model_name
        self.iteration = iteration

    def _sell_stock(self, index, action):
        # perform sell action based on the sign of the action
        if self.turbulence < self.turbulence_threshold:
            # check whether you have some stocks or not
            if self.state[index + STOCK_DIM + 1] > 0:
                # update balance by adding the money you get ater selling that particular stock
                self.state[0] += (
                    self.state[index + 1]
                    * min(abs(action), self.state[index + STOCK_DIM + 1])
                    * (1 - TRANSACTION_FEE_PERCENT)
                )
                # update the number of stocks
                self.state[index + STOCK_DIM + 1] -= min(
                    abs(action), self.state[index + STOCK_DIM + 1]
                )
                self.cost += (
                    self.state[index + 1]
                    * min(abs(action), self.state[index + STOCK_DIM + 1])
                    * TRANSACTION_FEE_PERCENT
                )
                self.trades += 1
            else:
                pass
        else:
            # if turbulence goes over threshold, just clear out all positions
            if self.state[index + STOCK_DIM + 1] > 0:
                # update balance
                self.state[0] += (
                    self.state[index + 1]
                    * self.state[index + STOCK_DIM + 1]
                    * (1 - TRANSACTION_FEE_PERCENT)
                )
                self.state[index + STOCK_DIM + 1] = 0
                self.cost += (
                    self.state[index + 1]
                    * self.state[index + STOCK_DIM + 1]
                    * TRANSACTION_FEE_PERCENT
                )
                self.trades += 1
            else:
                pass

    def _buy_stock(self, index, action):
        # perform buy action based on the sign of the action
        if self.turbulence < self.turbulence_threshold:
            available_amount = self.state[0] // self.state[index + 1]
            # print('available_amount:{}'.format(available_amount))
            # update balance
            self.state[0] -= (
                self.state[index + 1]
                * min(available_amount, action)
                * (1 + TRANSACTION_FEE_PERCENT)
            )

            self.state[index + STOCK_DIM + 1] += min(available_amount, action)

            self.cost += (
                self.state[index + 1]
                * min(available_amount, action)
                * TRANSACTION_FEE_PERCENT
            )
            self.trades += 1
        else:
            # if turbulence goes over threshold, just stop buying
            pass

    def step(self, actions):
        # print(self.day)
        self.terminal = self.day >= len(self.all_data) - 1
        # print(actions)

        if self.terminal:
            print("Reached the end.")
            if not os.path.exists("results"):
                os.makedirs("results")
            plt.plot(self.asset_memory, "r")
            plt.savefig(
                "results/account_value_trade_{}_{}.png".format(
                    self.model_name, self.iteration
                )
            )
            plt.close()
            df_total_value = pd.DataFrame(self.asset_memory)
            df_total_value.to_csv(
                "results/account_value_trade_{}_{}.csv".format(
                    self.model_name, self.iteration
                )
            )
            end_total_asset = self.state[0] + sum(
                np.array(self.state[HOLDING_IDX:EMB_IDX])
                * np.array(self.state[TARGET_IDX:PRICEDIFF_IDX])
            )
            print("previous_total_asset:{}".format(self.asset_memory[0]))

            print("end_total_asset:{}".format(end_total_asset))
            print("total_reward:{}".format(end_total_asset - self.asset_memory[0]))
            print("total_cost: ", self.cost)
            print("total trades: ", self.trades)

            df_total_value.columns = ["account_value"]
            df_total_value["daily_return"] = df_total_value.pct_change(1)

            cum_returns = (
                (end_total_asset - self.asset_memory[0]) / (self.asset_memory[0])
            ) * 100
            sharpe = sharpe_ratio(df_total_value["daily_return"])
            sortino = sortino_ratio(df_total_value["daily_return"])
            calmar = calmar_ratio(df_total_value["daily_return"])
            mdd = max_drawdown(df_total_value["daily_return"]) * 100

            df_rewards = pd.DataFrame(self.rewards_memory)
            df_rewards.to_csv(
                "results/account_rewards_trade_{}_{}.csv".format(
                    self.model_name, self.iteration
                )
            )

            return (
                self.state,
                self.reward,
                self.terminal,
                {
                    "sharpe": sharpe,
                    "sortino": sortino,
                    "calmar": calmar,
                    "mdd": mdd,
                    "cum_returns": cum_returns,
                },
            )

        else:
            actions = actions * HMAX_NORMALIZE
            if self.turbulence >= self.turbulence_threshold:
                actions = np.array([-HMAX_NORMALIZE] * STOCK_DIM)

            argsort_actions = np.argsort(actions)

            sell_index = argsort_actions[: np.where(actions < 0)[0].shape[0]]
            buy_index = argsort_actions[::-1][: np.where(actions > 0)[0].shape[0]]

            begin_total_asset = np.array(self.state[HOLDING_IDX:EMB_IDX]) * np.array(
                self.state[LAST_PRICE_IDX:HOLDING_IDX]
            )
            for index in sell_index:
                # print('take sell action'.format(actions[index]))
                self._sell_stock(index, actions[index])

            for index in buy_index:
                # print('take buy action: {}'.format(actions[index]))
                self._buy_stock(index, actions[index])

            end_total_asset = np.array(self.state[HOLDING_IDX:EMB_IDX]) * np.array(
                self.state[TARGET_IDX:PRICEDIFF_IDX]
            )

            self.asset_memory.append(self.state[0] + sum(end_total_asset))

            if self.args.diff == "price":
                self.reward = sum(
                    (end_total_asset - begin_total_asset)
                    * np.array(self.state[PRICEDIFF_IDX:VOLDIFF_IDX])
                )
                self.rewards_memory.append(self.reward)
                self.reward = self.reward * REWARD_SCALING
            elif self.args.diff == "vol":
                self.reward = sum(
                    (end_total_asset - begin_total_asset)
                    * np.array(self.state[VOLDIFF_IDX:TEXTDIFF_IDX])
                )
                self.rewards_memory.append(self.reward)
                self.reward = self.reward * REWARD_SCALING
            elif self.args.diff == "text":
                self.reward = sum(
                    (end_total_asset - begin_total_asset)
                    * np.array(self.state[TEXTDIFF_IDX:PRICE_TEXT_DIFF_IDX])
                )
                self.rewards_memory.append(self.reward)
                self.reward = self.reward * REWARD_SCALING
            elif self.args.diff == "price_text":
                self.reward = sum(
                    (end_total_asset - begin_total_asset)
                    * np.array(self.state[PRICE_TEXT_DIFF_IDX:ALLDIFF_IDX])
                )
                self.rewards_memory.append(self.reward)
                self.reward = self.reward * REWARD_SCALING
            elif self.args.diff == "pvt":
                self.reward = sum(
                    (end_total_asset - begin_total_asset)
                    * np.array(self.state[ALLDIFF_IDX:TIME_IDX])
                )
                self.rewards_memory.append(self.reward)
                self.reward = self.reward * REWARD_SCALING
            else:
                self.reward = sum(end_total_asset - begin_total_asset)
                self.rewards_memory.append(self.reward)
                self.reward = self.reward * REWARD_SCALING

            self.day += 1
            self.data = self.all_data[self.day]

            last_price = self.data["adj_close_last"].view(-1).tolist()
            target_price = self.data["adj_close_target"].view(-1).tolist()
            len_data = self.data["length_data"].view(-1).tolist()
            emb_data = self.data["embedding"].view(-1).tolist()
            text_diff = self.data["text_difficulty"].view(-1).tolist()
            vol_diff = self.data["volatility"].view(-1).tolist()
            price_text_diff = self.data["price_text_difficulty"].view(-1).tolist()
            price_diff = self.data["price_difficulty"].view(-1).tolist()
            all_diff = self.data["price_text_vol_difficulty"].view(-1).tolist()
            time_feats = self.data["time_features"].view(-1).tolist()
            self.state = (
                [self.state[0]]  # balance
                + last_price  # stock prices initial
                + list(self.state[(STOCK_DIM + 1) : (STOCK_DIM * 2 + 1)])
                + emb_data  # tweet features
                + len_data  # tweet len
                + target_price  # target price
                + price_diff
                + vol_diff
                + text_diff
                + price_text_diff
                + all_diff
                + time_feats
            )

        return self.state, self.reward, self.terminal, {}

    def reset(self):
        if self.initial:
            self.asset_memory = [INITIAL_ACCOUNT_BALANCE]
            self.day = 0
            self.data = self.all_data[self.day]
            self.turbulence = 0
            self.cost = 0
            self.trades = 0
            self.terminal = False
            # self.iteration=self.iteration
            self.rewards_memory = []
            # initiate state
            last_price = self.data["adj_close_last"].view(-1).tolist()
            target_price = self.data["adj_close_target"].view(-1).tolist()
            len_data = self.data["length_data"].view(-1).tolist()
            emb_data = self.data["embedding"].view(-1).tolist()
            text_diff = self.data["text_difficulty"].view(-1).tolist()
            vol_diff = self.data["volatility"].view(-1).tolist()
            price_text_diff = self.data["price_text_difficulty"].view(-1).tolist()
            price_diff = self.data["price_difficulty"].view(-1).tolist()
            all_diff = self.data["price_text_vol_difficulty"].view(-1).tolist()
            time_feats = self.data["time_features"].view(-1).tolist()
            self.state = (
                [INITIAL_ACCOUNT_BALANCE]  # balance
                + last_price  # stock prices initial
                + [0] * STOCK_DIM  # stocks on hold
                + emb_data  # tweet features
                + len_data  # tweet len
                + target_price  # target price
                + price_diff
                + vol_diff
                + text_diff
                + price_text_diff
                + all_diff
                + time_feats
            )
        else:
            previous_total_asset = self.previous_state[0] + sum(
                np.array(self.previous_state[1 : (STOCK_DIM + 1)])
                * np.array(self.previous_state[(STOCK_DIM + 1) : (STOCK_DIM * 2 + 1)])
            )
            self.asset_memory = [previous_total_asset]
            # self.asset_memory = [self.previous_state[0]]
            self.day = 0
            self.data = self.all_data[self.day]
            self.turbulence = 0
            self.cost = 0
            self.trades = 0
            self.terminal = False
            # self.iteration=iteration
            self.rewards_memory = []
            last_price = self.data["adj_close_last"].view(-1).tolist()
            target_price = self.data["adj_close_target"].view(-1).tolist()
            len_data = self.data["length_data"].view(-1).tolist()
            emb_data = self.data["embedding"].view(-1).tolist()
            text_diff = self.data["text_difficulty"].view(-1).tolist()
            vol_diff = self.data["volatility"].view(-1).tolist()
            price_text_diff = self.data["price_text_difficulty"].view(-1).tolist()
            price_diff = self.data["price_difficulty"].view(-1).tolist()
            all_diff = self.data["price_text_vol_difficulty"].view(-1).tolist()
            time_feats = self.data["time_features"].view(-1).tolist()
            self.state = (
                [self.previous_state[0]]  # balance
                + last_price  # stock prices initial
                # stocks on hold
                + self.previous_state[HOLDING_IDX:EMB_IDX]
                + emb_data  # tweet features
                + len_data  # tweet len
                + target_price  # target price
                + price_diff
                + vol_diff
                + text_diff
                + price_text_diff
                + all_diff
                + time_feats
            )

        return self.state

    def render(self, mode="human", close=False):
        return self.state

    def _seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

================
File: requirements.txt
================
absl-py==0.12.0
alabaster==0.7.12
albumentations==0.1.12
altair==4.1.0
appdirs==1.4.4
argon2-cffi==20.1.0
arviz==0.11.2
astor==0.8.1
astropy==4.2.1
astunparse==1.6.3
async-generator==1.10
atari-py==0.2.9
atomicwrites==1.4.0
attrs==21.2.0
audioread==2.1.9
autograd==1.3
Babel==2.9.1
backcall==0.2.0
beautifulsoup4==4.6.3
bleach==3.3.0
blis==0.4.1
bokeh==2.3.2
Bottleneck==1.3.2
branca==0.4.2
bs4==0.0.1
CacheControl==0.12.6
cached-property==1.5.2
cachetools==4.2.2
catalogue==1.0.0
certifi==2021.5.30
cffi==1.14.5
cftime==1.5.0
chardet==3.0.4
click==7.1.2
cloudpickle==1.3.0
cmake==3.12.0
cmdstanpy==0.9.5
colorcet==2.0.6
colorlover==0.3.0
community==1.0.0b1
contextlib2==0.5.5
convertdate==2.3.2
coverage==3.7.1
coveralls==0.5
crcmod==1.7
cufflinks==0.17.3
cupy-cuda101==9.1.0
cvxopt==1.2.6
cvxpy==1.0.31
cycler==0.10.0
cymem==2.0.5
Cython==0.29.23
daft==0.0.4
dask==2.12.0
datascience==0.10.6
debugpy==1.0.0
decorator==4.4.2
defusedxml==0.7.1
descartes==1.1.0
dill==0.3.4
distributed==1.25.3
dlib==19.18.0
dm-tree==0.1.6
docopt==0.6.2
docutils==0.17.1
dopamine-rl==1.0.5
earthengine-api==0.1.272
easydict==1.9
ecos==2.0.7.post1
editdistance==0.5.3
empyrical==0.5.5
en-core-web-sm==2.2.5
entrypoints==0.3
ephem==4.0.0.2
et-xmlfile==1.1.0
fa2==0.3.5
fastai==1.0.61
fastdtw==0.3.4
fastprogress==1.0.0
fastrlock==0.6
fbprophet==0.7.1
feather-format==0.4.1
filelock==3.0.12
firebase-admin==4.4.0
fix-yahoo-finance==0.0.22
Flask==1.1.4
flatbuffers==1.12
folium==0.8.3
future==0.16.0
gast==0.4.0
GDAL==2.2.2
gdown==3.6.4
gensim==3.6.0
geographiclib==1.52
geopy==1.17.0
gin-config==0.4.0
glob2==0.7
google==2.0.3
google-api-core==1.26.3
google-api-python-client==1.12.8
google-auth==1.32.1
google-auth-httplib2==0.0.4
google-auth-oauthlib==0.4.4
google-colab==1.0.0
google-resumable-media==0.4.1
googleapis-common-protos==1.53.0
googledrivedownloader==0.4
graphviz==0.10.1
greenlet==1.1.0
grpcio==1.34.1
gspread==3.0.1
gspread-dataframe==3.0.8
gym==0.17.3
h5py==3.1.0
HeapDict==1.0.1
hijri-converter==2.1.3
holidays==0.10.5.2
holoviews==1.14.4
html5lib==1.0.1
httpimport==0.5.18
httplib2==0.17.4
httplib2shim==0.0.3
humanize==0.5.1
hyperopt==0.1.2
ideep4py==2.0.0.post3
idna==2.10
imageio==2.4.1
imagesize==1.2.0
imbalanced-learn==0.4.3
imblearn==0.0
imgaug==0.2.9
importlib-metadata==4.6.0
importlib-resources==5.2.0
imutils==0.5.4
inflect==2.1.0
iniconfig==1.1.1
install==1.3.4
intel-openmp==2021.3.0
intervaltree==2.1.0
ipdb==0.13.9
ipykernel==4.10.1
ipython==7.25.0
ipython-genutils==0.2.0
ipython-sql==0.3.9
ipywidgets==7.6.3
itsdangerous==1.1.0
jax==0.2.13
jaxlib==0.1.66+cuda110
jdcal==1.4.1
jedi==0.18.0
jieba==0.42.1
Jinja2==2.11.3
joblib==1.0.1
jpeg4py==0.1.4
jsonschema==2.6.0
jupyter==1.0.0
jupyter-client==5.3.5
jupyter-console==5.2.0
jupyter-core==4.7.1
jupyterlab-pygments==0.1.2
jupyterlab-widgets==1.0.0
kaggle==1.5.12
kapre==0.3.5
Keras==2.4.3
keras-nightly==2.5.0.dev2021032900
Keras-Preprocessing==1.1.2
keras-vis==0.4.1
kiwisolver==1.3.1
korean-lunar-calendar==0.2.1
librosa==0.8.1
lightgbm==2.2.3
llvmlite==0.34.0
lmdb==0.99
LunarCalendar==0.0.9
lxml==4.2.6
Markdown==3.3.4
MarkupSafe==2.0.1
matplotlib==3.2.2
matplotlib-inline==0.1.2
matplotlib-venn==0.11.6
missingno==0.4.2
mistune==0.8.4
mizani==0.6.0
mkl==2019.0
mlxtend==0.14.0
more-itertools==8.8.0
moviepy==0.2.3.5
mpmath==1.2.1
msgpack==1.0.2
multiprocess==0.70.12.2
multitasking==0.0.9
murmurhash==1.0.5
music21==5.5.0
natsort==5.5.0
nbclient==0.5.3
nbconvert==5.6.1
nbformat==5.1.3
nest-asyncio==1.5.1
netCDF4==1.5.7
networkx==2.5.1
nibabel==3.0.2
nltk==3.2.5
notebook==5.3.1
numba==0.51.2
numexpr==2.7.3
numpy==1.19.5
nvidia-ml-py3==7.352.0
oauth2client==4.1.3
oauthlib==3.1.1
okgrade==0.4.3
opencv-contrib-python==4.1.2.30
opencv-python==4.1.2.30
openpyxl==2.5.9
opt-einsum==3.3.0
osqp==0.6.2.post0
packaging==20.9
palettable==3.3.0
pandas==1.1.5
pandas-datareader==0.9.0
pandas-gbq==0.13.3
pandas-profiling==1.4.1
pandocfilters==1.4.3
panel==0.11.3
param==1.10.1
parso==0.8.2
pathlib==1.0.1
patsy==0.5.1
pexpect==4.8.0
pickleshare==0.7.5
Pillow==7.1.2
pip-tools==4.5.1
plac==1.1.3
plotly==4.4.1
plotnine==0.6.0
pluggy==0.7.1
pooch==1.4.0
portpicker==1.3.9
prefetch-generator==1.0.1
preshed==3.0.5
prettytable==2.1.0
progressbar2==3.38.0
prometheus-client==0.11.0
promise==2.3
prompt-toolkit==3.0.19
protobuf==3.17.3
psutil==5.4.8
psycopg2==2.7.6.1
ptyprocess==0.7.0
py==1.10.0
pyarrow==3.0.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycocotools==2.0.2
pycparser==2.20
pyct==0.4.8
pydata-google-auth==1.2.0
pydot==1.3.0
pydot-ng==2.0.0
pydotplus==2.0.2
PyDrive==1.3.1
pyemd==0.5.1
pyerfa==2.0.0
pyfolio==0.9.2
pyglet==1.5.0
Pygments==2.6.1
pygobject==3.26.1
pymc3==3.11.2
PyMeeus==0.5.11
pymongo==3.11.4
pymystem3==0.2.0
PyOpenGL==3.1.5
pyparsing==2.4.7
pyrsistent==0.18.0
pysndfile==1.3.8
PySocks==1.7.1
pystan==2.19.1.1
pytest==3.6.4
python-apt==0.0.0
python-chess==0.23.11
python-dateutil==2.8.1
python-louvain==0.15
python-slugify==5.0.2
python-utils==2.5.6
pytz==2018.9
pyviz-comms==2.1.0
PyWavelets==1.1.1
PyYAML==3.13
pyzmq==22.1.0
qdldl==0.1.5.post0
qtconsole==5.1.1
QtPy==1.9.0
regex==2019.12.20
requests==2.23.0
requests-oauthlib==1.3.0
resampy==0.2.2
retrying==1.3.3
rpy2==3.4.5
rsa==4.7.2
scikit-image==0.16.2
scikit-learn==0.22.2.post1
scipy==1.4.1
screen-resolution-extra==0.0.0
scs==2.1.4
seaborn==0.11.1
semver==2.13.0
Send2Trash==1.7.1
setuptools-git==1.2
Shapely==1.7.1
simplegeneric==0.8.1
six==1.15.0
sklearn==0.0
sklearn-pandas==1.8.0
smart-open==5.1.0
snowballstemmer==2.1.0
sortedcontainers==2.4.0
SoundFile==0.10.3.post1
spacy==2.2.4
Sphinx==1.8.5
sphinxcontrib-serializinghtml==1.1.5
sphinxcontrib-websupport==1.2.4
SQLAlchemy==1.4.20
sqlparse==0.4.1
srsly==1.0.5
statsmodels==0.10.2
sympy==1.7.1
tables==3.4.4
tabulate==0.8.9
tblib==1.7.0
tensorboard==2.5.0
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.0
tensorflow==2.5.0
tensorflow-datasets==4.0.1
tensorflow-estimator==2.5.0
tensorflow-gcs-config==2.5.0
tensorflow-hub==0.12.0
tensorflow-metadata==1.1.0
tensorflow-probability==0.13.0
termcolor==1.1.0
terminado==0.10.1
testpath==0.5.0
text-unidecode==1.3
textblob==0.15.3
Theano-PyMC==1.1.2
thinc==7.4.0
tifffile==2021.7.2
toml==0.10.2
toolz==0.11.1
torch==1.9.0+cu102
torchsummary==1.5.1
torchtext==0.10.0
torchvision==0.10.0+cu102
tornado==5.1.1
tqdm==4.41.1
traitlets==5.0.5
tweepy==3.10.0
typeguard==2.7.1
typing-extensions==3.7.4.3
tzlocal==1.5.1
uritemplate==3.0.1
urllib3==1.24.3
vega-datasets==0.9.0
wasabi==0.8.2
wcwidth==0.2.5
webencodings==0.5.1
Werkzeug==1.0.1
widgetsnbextension==3.5.1
wordcloud==1.5.0
wrapt==1.12.1
xarray==0.18.2
xgboost==0.90
xkit==0.0.0
xlrd==1.1.0
xlwt==1.3.0
yellowbrick==0.9.1
zict==2.0.0
zipp==3.4.1

================
File: memory.py
================
from __future__ import absolute_import
from collections import deque, namedtuple
import warnings
import random
import numpy as np

# This is to be understood as a transition: Given `state0`, performing `action`
# yields `reward` and results in `state1`, which might be `terminal`.
Experience = namedtuple("Experience", "state0, action, reward, state1, terminal1")

def sample_batch_indexes(low, high, size):
    if high - low >= size:
        try:
            r = xrange(low, high)
        except NameError:
            r = range(low, high)
        batch_idxs = random.sample(r, size)
    else:
        warnings.warn(
            "Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!"
        )
        batch_idxs = np.random.random_integers(low, high - 1, size=size)
    assert len(batch_idxs) == size
    return batch_idxs

class RingBuffer(object):
    def __init__(self, maxlen):
        self.maxlen = maxlen
        self.start = 0
        self.length = 0
        self.data = [None for _ in range(maxlen)]

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        if idx < 0 or idx >= self.length:
            raise KeyError()
        return self.data[(self.start + idx) % self.maxlen]

    def append(self, v):
        if self.length < self.maxlen:
            # We have space, simply increase the length.
            self.length += 1
        elif self.length == self.maxlen:
            # No space, "remove" the first item.
            self.start = (self.start + 1) % self.maxlen
        else:
            # This should never happen.
            raise RuntimeError()
        self.data[(self.start + self.length - 1) % self.maxlen] = v

def zeroed_observation(observation):
    if hasattr(observation, "shape"):
        return np.zeros(observation.shape)
    elif hasattr(observation, "__iter__"):
        out = []
        for x in observation:
            out.append(zeroed_observation(x))
        return out
    else:
        return 0.0

class Memory(object):
    def __init__(self, window_length, ignore_episode_boundaries=False):
        self.window_length = window_length
        self.ignore_episode_boundaries = ignore_episode_boundaries

        self.recent_observations = deque(maxlen=window_length)
        self.recent_terminals = deque(maxlen=window_length)

    def sample(self, batch_size, batch_idxs=None):
        raise NotImplementedError()

    def append(self, observation, action, reward, terminal, training=True):
        self.recent_observations.append(observation)
        self.recent_terminals.append(terminal)

    def get_recent_state(self, current_observation):
        state = [current_observation]
        idx = len(self.recent_observations) - 1
        for offset in range(0, self.window_length - 1):
            current_idx = idx - offset
            current_terminal = (
                self.recent_terminals[current_idx - 1]
                if current_idx - 1 >= 0
                else False
            )
            if current_idx < 0 or (
                not self.ignore_episode_boundaries and current_terminal
            ):
                # The previously handled observation was terminal, don't add the current one.
                # Otherwise we would leak into a different episode.
                break
            state.insert(0, self.recent_observations[current_idx])
        while len(state) < self.window_length:
            state.insert(0, zeroed_observation(state[0]))
        return state

    def get_config(self):
        config = {
            "window_length": self.window_length,
            "ignore_episode_boundaries": self.ignore_episode_boundaries,
        }
        return config


class SequentialMemory(Memory):
    def __init__(self, limit, **kwargs):
        super(SequentialMemory, self).__init__(**kwargs)

        self.limit = limit

        # Do not use deque to implement the memory. This data structure may seem convenient but
        # it is way too slow on random access. Instead, we use our own ring buffer implementation.
        self.actions = RingBuffer(limit)
        self.rewards = RingBuffer(limit)
        self.terminals = RingBuffer(limit)
        self.observations = RingBuffer(limit)

    def sample(self, batch_size, batch_idxs=None):
        if batch_idxs is None:
            # Draw random indexes such that we have at least a single entry before each
            # index.
            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)
        batch_idxs = np.array(batch_idxs) + 1
        assert np.min(batch_idxs) >= 1
        assert np.max(batch_idxs) < self.nb_entries
        assert len(batch_idxs) == batch_size

        # Create experiences
        experiences = []
        for idx in batch_idxs:
            terminal0 = self.terminals[idx - 2] if idx >= 2 else False
            while terminal0:
                # Skip this transition because the environment was reset here. Select a new, random
                # transition and use this instead. This may cause the batch to contain the same
                # transition twice.
                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]
                terminal0 = self.terminals[idx - 2] if idx >= 2 else False
            assert 1 <= idx < self.nb_entries

            # This code is slightly complicated by the fact that subsequent observations might be
            # from different episodes. We ensure that an experience never spans multiple episodes.
            # This is probably not that important in practice but it seems cleaner.
            state0 = [self.observations[idx - 1]]
            for offset in range(0, self.window_length - 1):
                current_idx = idx - 2 - offset
                current_terminal = (
                    self.terminals[current_idx - 1] if current_idx - 1 > 0 else False
                )
                if current_idx < 0 or (
                    not self.ignore_episode_boundaries and current_terminal
                ):
                    # The previously handled observation was terminal, don't add the current one.
                    # Otherwise we would leak into a different episode.
                    break
                state0.insert(0, self.observations[current_idx])
            while len(state0) < self.window_length:
                state0.insert(0, zeroed_observation(state0[0]))
            action = self.actions[idx - 1]
            reward = self.rewards[idx - 1]
            terminal1 = self.terminals[idx - 1]

            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep
            # to the right. Again, we need to be careful to not include an observation from the next
            # episode if the last state is terminal.
            state1 = [np.copy(x) for x in state0[1:]]
            state1.append(self.observations[idx])

            assert len(state0) == self.window_length
            assert len(state1) == len(state0)
            experiences.append(
                Experience(
                    state0=state0,
                    action=action,
                    reward=reward,
                    state1=state1,
                    terminal1=terminal1,
                )
            )
        assert len(experiences) == batch_size
        return experiences

    def sample_and_split(self, batch_size, batch_idxs=None):
        experiences = self.sample(batch_size, batch_idxs)

        state0_batch = []
        reward_batch = []
        action_batch = []
        terminal1_batch = []
        state1_batch = []
        for e in experiences:
            state0_batch.append(e.state0)
            state1_batch.append(e.state1)
            reward_batch.append(e.reward)
            action_batch.append(e.action)
            terminal1_batch.append(0.0 if e.terminal1 else 1.0)

        # Prepare and validate parameters.
        state0_batch = np.array(state0_batch).reshape(batch_size, -1)
        state1_batch = np.array(state1_batch).reshape(batch_size, -1)
        terminal1_batch = np.array(terminal1_batch).reshape(batch_size, -1)
        reward_batch = np.array(reward_batch).reshape(batch_size, -1)
        action_batch = np.array(action_batch).reshape(batch_size, -1)

        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch

    def append(self, observation, action, reward, terminal, training=True):
        super(SequentialMemory, self).append(
            observation, action, reward, terminal, training=training
        )

        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`
        # and weather the next state is `terminal` or not.
        if training:
            self.observations.append(observation)
            self.actions.append(action)
            self.rewards.append(reward)
            self.terminals.append(terminal)

    @property
    def nb_entries(self):
        return len(self.observations)

    def get_config(self):
        config = super(SequentialMemory, self).get_config()
        config["limit"] = self.limit
        return config


class EpisodeParameterMemory(Memory):
    def __init__(self, limit, **kwargs):
        super(EpisodeParameterMemory, self).__init__(**kwargs)
        self.limit = limit

        self.params = RingBuffer(limit)
        self.intermediate_rewards = []
        self.total_rewards = RingBuffer(limit)

    def sample(self, batch_size, batch_idxs=None):
        if batch_idxs is None:
            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)
        assert len(batch_idxs) == batch_size

        batch_params = []
        batch_total_rewards = []
        for idx in batch_idxs:
            batch_params.append(self.params[idx])
            batch_total_rewards.append(self.total_rewards[idx])
        return batch_params, batch_total_rewards

    def append(self, observation, action, reward, terminal, training=True):
        super(EpisodeParameterMemory, self).append(
            observation, action, reward, terminal, training=training
        )
        if training:
            self.intermediate_rewards.append(reward)

    def finalize_episode(self, params):
        total_reward = sum(self.intermediate_rewards)
        self.total_rewards.append(total_reward)
        self.params.append(params)
        self.intermediate_rewards = []

    @property
    def nb_entries(self):
        return len(self.total_rewards)

    def get_config(self):
        config = super(SequentialMemory, self).get_config()
        config["limit"] = self.limit
        return config

================
File: util.py
================
import os
import torch
from torch.autograd import Variable

USE_CUDA = torch.cuda.is_available()
FLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor

def prRed(prt):
    print("\033[91m {}\033[00m".format(prt))

def prGreen(prt):
    print("\033[92m {}\033[00m".format(prt))

def prYellow(prt):
    print("\033[93m {}\033[00m".format(prt))

def prLightPurple(prt):
    print("\033[94m {}\033[00m".format(prt))

def prPurple(prt):
    print("\033[95m {}\033[00m".format(prt))

def prCyan(prt):
    print("\033[96m {}\033[00m".format(prt))

def prLightGray(prt):
    print("\033[97m {}\033[00m".format(prt))

def prBlack(prt):
    print("\033[98m {}\033[00m".format(prt))

def to_numpy(var):
    return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()

def to_tensor(ndarray, volatile=False, requires_grad=False, dtype=FLOAT):
    return Variable(
        torch.from_numpy(ndarray), volatile=volatile, requires_grad=requires_grad
    ).type(dtype)

def soft_update(target, source, tau):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

def hard_update(target, source):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(param.data)

def get_output_folder(parent_dir, env_name):
    os.makedirs(parent_dir, exist_ok=True)
    experiment_id = 0
    for folder_name in os.listdir(parent_dir):
        if not os.path.isdir(os.path.join(parent_dir, folder_name)):
            continue
        try:
            folder_name = int(folder_name.split("-run")[-1])
            if folder_name > experiment_id:
                experiment_id = folder_name
        except:
            pass
    experiment_id += 1

    parent_dir = os.path.join(parent_dir, env_name)
    parent_dir = parent_dir + "-run{}".format(experiment_id)
    os.makedirs(parent_dir, exist_ok=True)
    return parent_dir

================
File: model.py
================
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from ipdb import set_trace as debug
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from torch.autograd import Variable
from configs_stock import *

device = torch.device("cpu")
class TimeLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, cuda_flag=False, bidirectional=False):
        super(TimeLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.input_size = input_size
        self.cuda_flag = cuda_flag
        self.W_all = nn.Linear(hidden_size, hidden_size * 4)
        self.U_all = nn.Linear(input_size, hidden_size * 4)
        self.W_d = nn.Linear(hidden_size, hidden_size)
        self.bidirectional = bidirectional

    def forward(self, inputs, timestamps, hidden_states, reverse=False):

        b, seq, embed = inputs.size()
        h = hidden_states[0]
        c = hidden_states[1]

        if self.cuda_flag:
            h = h.cuda()
            c = c.cuda()
        outputs = []
        hidden_state_h = []
        hidden_state_c = []

        for s in range(seq):
            c_s1 = torch.tanh(self.W_d(c))  # short term mem
            # discounted short term mem
            c_s2 = c_s1 * timestamps[:, s: s + 1].expand_as(c_s1)
            c_l = c - c_s1  # long term mem
            c_adj = c_l + c_s2  # adjusted = long + disc short term mem
            outs = self.W_all(h) + self.U_all(inputs[:, s])
            f, i, o, c_tmp = torch.chunk(outs, 4, 1)
            f = torch.sigmoid(f)
            i = torch.sigmoid(i)
            o = torch.sigmoid(o)
            c_tmp = torch.sigmoid(c_tmp)
            c = f * c_adj + i * c_tmp
            h = o * torch.tanh(c)
            outputs.append(o)
            hidden_state_c.append(c)
            hidden_state_h.append(h)

        if reverse:
            outputs.reverse()
            hidden_state_c.reverse()
            hidden_state_h.reverse()

        outputs = torch.stack(outputs, 1)
        hidden_state_c = torch.stack(hidden_state_c, 1)
        hidden_state_h = torch.stack(hidden_state_h, 1)

        return outputs, (h, c)

class attn(torch.nn.Module):
    def __init__(self, in_shape, use_attention=True, maxlen=None):
        super(attn, self).__init__()
        self.use_attention = use_attention
        if self.use_attention:
            self.W1 = torch.nn.Linear(in_shape, in_shape).to(device)
            self.W2 = torch.nn.Linear(in_shape, in_shape).to(device)
            self.V = torch.nn.Linear(in_shape, 1).to(device)
        if maxlen != None:
            self.arange = torch.arange(maxlen).to(device)

    def forward(self, full, last, lens=None, dim=1):
        """
        full : B*30*in_shape
        last : B*1*in_shape
        lens: B*1
        """
        if self.use_attention:
            score = self.V(F.tanh(self.W1(last) + self.W2(full)))
            # print(score.shape) -> B*30*1

            if lens != None:
                mask = self.arange[None, :] < lens[:, None]  # B*30
                score[~mask] = float("-inf")

            attention_weights = F.softmax(score, dim=dim)
            context_vector = attention_weights * full
            context_vector = torch.sum(context_vector, dim=dim)
            return context_vector  # B*in_shape
        else:
            if lens != None:
                mask = self.arange[None, :] < lens[:, None]  # B*30
                mask = mask.type(torch.float).unsqueeze(-1).cuda()
                context_vector = full * mask
                context_vector = torch.mean(context_vector, dim=dim)
                return context_vector
            else:
                return torch.mean(full, dim=dim)

class Actor(nn.Module):
    """
    Actor:
        Gets the text: news/tweets about the stocks,
        current balance, price and holds on the stocks.
    """

    def __init__(
        self,
        num_stocks=STOCK_DIM,
        text_embed_dim=TWEETS_EMB,
        intraday_hiddenDim=128,
        interday_hiddenDim=128,
        intraday_numLayers=1,
        interday_numLayers=1,
        use_attn1=False,
        use_attn2=False,
        maxlen=30,
        device=torch.device("cpu"),
    ):
        """
        num_stocks: number of stocks for which the agent is trading
        """
        super(Actor, self).__init__()

        self.lstm1s = [
            TimeLSTM(
                input_size=text_embed_dim,
                hidden_size=intraday_hiddenDim,
            )
            for _ in range(num_stocks)
        ]

        for i, tweet_lstm in enumerate(self.lstm1s):
            self.add_module("lstm1_{}".format(i), tweet_lstm)

        self.lstm1_outshape = intraday_hiddenDim
        self.lstm2_outshape = interday_hiddenDim

        self.attn1s = [
            attn(self.lstm1_outshape, maxlen=maxlen)
            for _ in range(num_stocks)
        ]

        for i, tweet_attn in enumerate(self.attn1s):
            self.add_module("attn1_{}".format(i), tweet_attn)

        self.lstm2s = [
            nn.LSTM(
                input_size=self.lstm1_outshape,
                hidden_size=interday_hiddenDim,
                num_layers=interday_numLayers,
                batch_first=True,
                bidirectional=False,
            )
            for _ in range(num_stocks)
        ]
        for i, day_lstm in enumerate(self.lstm2s):
            self.add_module("lstm2_{}".format(i), day_lstm)

        self.attn2s = [
            attn(self.lstm2_outshape)
            for _ in range(num_stocks)
        ]

        for i, day_attn in enumerate(self.attn2s):
            self.add_module("attn2_{}".format(i), day_attn)

        self.linearx1 = [
            nn.Linear(self.lstm2_outshape, self.lstm2_outshape)
            for _ in range(num_stocks)
        ]
        for i, linear_x in enumerate(self.linearx1):
            self.add_module("linearx1_{}".format(i), linear_x)

        self.linearx2 = [nn.Linear(self.lstm2_outshape, 64)
                         for _ in range(num_stocks)]
        for i, linear_x in enumerate(self.linearx2):
            self.add_module("linearx2_{}".format(i), linear_x)

        self.drop = nn.Dropout(p=0.3)
        self.softmax1 = nn.Softmax(dim=2)
        self.softmax2 = nn.Softmax(dim=1)
        self.relu = nn.ReLU()

        self.device = device
        self.maxlen = maxlen

        self.linear1 = nn.Linear(2 * num_stocks + 1, 64)
        self.linear2 = nn.Linear(64, 32)
        self.linear_c = nn.Linear(64 * num_stocks + 32, num_stocks)
        self.tanh = nn.Tanh()
        self.num_stocks = num_stocks
        self.device = device

    def init_hidden(self):
        h = Variable(torch.zeros(self.bs, self.lstm1_outshape)).to(self.device)
        c = Variable(torch.zeros(self.bs, self.lstm2_outshape)).to(self.device)

        return (h, c)

    def forward(self, state):
        state = state.view(-1, FEAT_DIMS)
        stock_feats = state[:, 0: 2 * self.num_stocks + 1].view(
            -1, 2 * self.num_stocks + 1
        )
        sentence_feat = state[:, EMB_IDX:LEN_IDX].view(
            -1, self.num_stocks, N_DAYS, MAX_TWEETS, TWEETS_EMB
        )
        len_tweets = state[:,
                           LEN_IDX:TARGET_IDX].view(-1, self.num_stocks, N_DAYS)
        time_feats = state[:,
                           TIME_IDX:].view(-1, self.num_stocks, N_DAYS, MAX_TWEETS)

        self.bs = sentence_feat.size(0)
        sentence_feat = sentence_feat.permute(1, 2, 0, 3, 4)
        len_tweets = len_tweets.permute(1, 2, 0)
        time_feats = time_feats.permute(1, 2, 0, 3)

        num_days = N_DAYS
        text_out = torch.zeros(self.num_stocks, self.bs, 64).to(self.device)
        for i in range(self.num_stocks):
            h_init, c_init = self.init_hidden()

            lstm1_out = torch.zeros(num_days, self.bs, self.lstm1_outshape).to(
                self.device
            )
            for j in range(num_days):
                temp_sent = sentence_feat[i, j, :, :, :]
                temp_len = len_tweets[i, j, :]
                temp_timefeats = time_feats[i, j, :, :]

                temp_lstmout, (_, _) = self.lstm1s[i](
                    temp_sent, temp_timefeats, (h_init, c_init)
                )

                last_idx = temp_len.type(torch.int).tolist()
                temp_hn = torch.zeros(self.bs, self.lstm1_outshape).to(self.device)
                for k in range(self.bs):
                    if last_idx[k] != 0:
                        temp_hn[k] = temp_lstmout[k, last_idx[k] - 1, :]
                lstm1_out[j] = self.attn1s[i](temp_lstmout, temp_hn, temp_len.to(self.device))

            lstm1_out = lstm1_out.permute(1, 0, 2)
            lstm2_out, (h2_out, _) = self.lstm2s[i](lstm1_out)
            h2_out = h2_out.permute(1, 0, 2)
            x = self.attn2s[i](lstm2_out, h2_out)
            x = self.drop(self.relu(self.linearx1[i](x)))
            x = self.linearx2[i](x)
            text_out[i] = x

        text_out = text_out.permute(1, 0, 2)
        text_out = text_out.view(self.bs, -1)
        x_stock = self.relu(self.linear1(stock_feats))
        x_stock = self.linear2(x_stock)

        full = torch.cat([x_stock, text_out], dim=1)
        full = self.tanh(self.linear_c(full))
        return full

class Critic(nn.Module):
    """
    Actor:
        Gets the text tweets about the stocks,
        current balance, price and holds on the stocks.
    """

    def __init__(
        self,
        num_stocks=STOCK_DIM,
        text_embed_dim=TWEETS_EMB,
        intraday_hiddenDim=128,
        interday_hiddenDim=128,
        intraday_numLayers=1,
        interday_numLayers=1,
        use_attn1=False,
        use_attn2=False,
        maxlen=30,
        device=torch.device("cpu"),
    ):
        """
        num_stocks: number of stocks for which the agent is trading
        """
        super(Critic, self).__init__()

        self.lstm1s = [
            TimeLSTM(
                input_size=text_embed_dim,
                hidden_size=intraday_hiddenDim,
            )
            for _ in range(num_stocks)
        ]

        for i, tweet_lstm in enumerate(self.lstm1s):
            self.add_module("lstm1_{}".format(i), tweet_lstm)

        self.lstm1_outshape = intraday_hiddenDim
        self.lstm2_outshape = interday_hiddenDim

        self.attn1s = [
            attn(self.lstm1_outshape, maxlen=maxlen)
            for _ in range(num_stocks)
        ]

        for i, tweet_attn in enumerate(self.attn1s):
            self.add_module("attn1_{}".format(i), tweet_attn)

        self.lstm2s = [
            nn.LSTM(
                input_size=self.lstm1_outshape,
                hidden_size=interday_hiddenDim,
                num_layers=interday_numLayers,
                batch_first=True,
                bidirectional=False,
            )
            for _ in range(num_stocks)
        ]
        for i, day_lstm in enumerate(self.lstm2s):
            self.add_module("lstm2_{}".format(i), day_lstm)

        self.attn2s = [
            attn(self.lstm2_outshape)
            for _ in range(num_stocks)
        ]

        for i, day_attn in enumerate(self.attn2s):
            self.add_module("attn2_{}".format(i), day_attn)

        self.linearx1 = [
            nn.Linear(self.lstm2_outshape, self.lstm2_outshape)
            for _ in range(num_stocks)
        ]
        for i, linear_x in enumerate(self.linearx1):
            self.add_module("linearx1_{}".format(i), linear_x)

        self.linearx2 = [nn.Linear(self.lstm2_outshape, 64)
                         for _ in range(num_stocks)]
        for i, linear_x in enumerate(self.linearx2):
            self.add_module("linearx2_{}".format(i), linear_x)

        self.drop = nn.Dropout(p=0.3)
        self.tanh = nn.Tanh()
        self.relu = nn.ReLU()
        self.num_stocks = num_stocks
        self.softmax1 = nn.Softmax(dim=2)
        self.softmax2 = nn.Softmax(dim=1)

        self.device = device
        self.maxlen = maxlen

        self.linear1 = nn.Linear(2 * num_stocks + 1, 64)
        self.linear2 = nn.Linear(64, 32)

        self.linear_c = nn.Linear(64 * num_stocks + 32, 32)

        # * Critic Layers
        self.linear_critic = nn.Linear(num_stocks, 32)

        # * Actions and States
        self.linear_sa1 = nn.Linear(64, 32)
        self.linear_sa2 = nn.Linear(32, 1)
        self.device = device
    def init_hidden(self):
        h = Variable(torch.zeros(self.bs, self.lstm1_outshape)).to(self.device)
        c = Variable(torch.zeros(self.bs, self.lstm2_outshape)).to(self.device)

        return (h, c)

    def forward(self, state, actions):
        state = state.view(-1, FEAT_DIMS)
        actions = actions.view(-1, STOCK_DIM)

        stock_feats = state[:, 0: 2 * self.num_stocks + 1].view(
            -1, 2 * self.num_stocks + 1
        )
        sentence_feat = state[:, EMB_IDX:LEN_IDX].view(
            -1, self.num_stocks, N_DAYS, MAX_TWEETS, TWEETS_EMB
        )
        len_tweets = state[:,
                           LEN_IDX:TARGET_IDX].view(-1, self.num_stocks, N_DAYS)
        time_feats = state[:,
                           TIME_IDX:].view(-1, self.num_stocks, N_DAYS, MAX_TWEETS)

        self.bs = sentence_feat.size(0)
        sentence_feat = sentence_feat.permute(1, 2, 0, 3, 4)
        len_tweets = len_tweets.permute(1, 2, 0)
        time_feats = time_feats.permute(1, 2, 0, 3)
        num_days = N_DAYS
        text_out = torch.zeros(self.num_stocks, self.bs, 64).to(self.device)
        for i in range(self.num_stocks):
            h_init, c_init = self.init_hidden()
            lstm1_out = torch.zeros(num_days, self.bs, self.lstm1_outshape).to(
                self.device
            )
            for j in range(num_days):
                temp_sent = sentence_feat[i, j, :, :, :]
                temp_len = len_tweets[i, j, :]
                temp_timefeats = time_feats[i, j, :, :]

                temp_lstmout, (_, _) = self.lstm1s[i](
                    temp_sent, temp_timefeats, (h_init, c_init)
                )
                last_idx = temp_len.type(torch.int).tolist()
                temp_hn = torch.zeros(self.bs, self.lstm1_outshape).to(self.device)
                for k in range(self.bs):
                    if last_idx[k] != 0:
                        temp_hn[k] = temp_lstmout[k, last_idx[k] - 1, :]

                lstm1_out[j] = self.attn1s[i](temp_lstmout, temp_hn, temp_len.to(self.device))

            lstm1_out = lstm1_out.permute(1, 0, 2)
            lstm2_out, (h2_out, _) = self.lstm2s[i](lstm1_out)
            h2_out = h2_out.permute(1, 0, 2)
            x = self.attn2s[i](lstm2_out, h2_out)

            x = self.drop(self.relu(self.linearx1[i](x)))
            x = self.linearx2[i](x)
            text_out[i] = x

        text_out = text_out.permute(1, 0, 2)
        text_out = text_out.view(self.bs, -1)

        x_stock = self.relu(self.linear1(stock_feats))
        x_stock = self.linear2(x_stock)

        full = torch.cat([x_stock, text_out], dim=1)
        full = self.linear_c(full)

        actions = self.linear_critic(actions)

        full = torch.cat([full, actions], dim=1)
        full = self.relu(self.linear_sa1(full))
        full = self.linear_sa2(full)

        return full

================
File: README.md
================
# Quantitative Day Trading from Natural Language using Reinforcement Learning

This codebase contains the python scripts for developing the model called PROFIT, proposed in the [paper](https://aclanthology.org/2021.naacl-main.316.pdf) titled "Quantitative Day Trading from Natural Language using Reinforcement Learning".

Published at NAACL - 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics

## Environment & Installation Steps
Create an environment and install the following dependencies
```python
pip install -r requirements.txt
```

## Contents and Description

configs_stock.py -> This file contains all the configuration parameters for the stock market environment.

ddpg.py -> This file contains an implementation of the Deep Deterministic Policy Gradient (DDPG) algorithm.

env.py -> This file contains the code for the stock market environment.

evaluator.py -> This file contains the implementation for evaluating the performance of PROFIT across different metrics.

main.py -> This file starts the training of the model.

memory.py -> This file implements the memory module for the DDPG algorithm.

model.py -> This file contains a heirarchical time-aware attention and LSTM based Actor & Critic model.

random_process.py -> This file contains implementation of some methods for random sampling for the DDPG algorithm.

util.py -> This file contains implementation of utility functions.

## Data and Preprocessing

Find the US S&P 500 dataset [here](https://github.com/yumoxu/stocknet-dataset), and the China & Hong Kong dataset [here](https://pan.baidu.com/s/1mhCLJJi).

To encode the texts, we use the 768-dimensional embedding obtained per news item or tweet by averaging the token-level outputs from the final layer of BERT. However, PROFIT is compatible with any and all 1-D text embeddings.
To extract the timestamp input for time-aware LSTM, we obtain the time interval (in minutes) between the release of two consecutive texts and compute its inverse.
Kindly refer the [paper](https://aclanthology.org/2021.naacl-main.316.pdf) for further pre-processing details and the model training setup.

Prepare two separate .pkl files, one for training and one for testing data, containing data processed as follows. Each data point should comprise values corresponding to the following keys for the set of stocks during the lookback window:

'dates' -> Dates (list) corresponding to the days in the lag window

'date_target' -> The date for the target trading day (trading day next to the last day in the lookback)

'date_last' -> The date for the last day in the lookback

'embedding' -> Data embeddings with dimensions as follows: `[number of stocks, length of lookback, number of financial texts per day, embedding size of financial text]`

'length_data' -> Denotes the number of texts for each day in the lag per stock, dimensions of tensor: `[number of stocks, length of lookback]`

'time_features' -> Inverse of time gap between release of two consecutive texts, dimensions of tensor: `[number of stocks, length of lookback, number of texts per day, 1]`

'adj_close_last' -> A list of adjusted closing prices for the set of stocks on the day before the target trading day

'adj_close_target' -> A list of adjusted closing prices for the set of stocks on the target trading day

## Training
Execute the following command in the same environment to train PROFIT:
```bash
python main.py --model profit --train_iter 1000
```

## Cite

If our work was helpful for your research, please don't forget to cite our work.

    @inproceedings{sawhney-etal-2021-quantitative,
        title = "Quantitative Day Trading from Natural Language using Reinforcement Learning",
        author = "Sawhney, Ramit  and
          Wadhwa, Arnav  and
          Agarwal, Shivam  and
          Shah, Rajiv Ratn",
        booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
        month = jun,
        year = "2021",
        address = "Online",
        publisher = "Association for Computational Linguistics",
        url = "https://aclanthology.org/2021.naacl-main.316",
        doi = "10.18653/v1/2021.naacl-main.316",
        pages = "4018--4030",
        abstract = "It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market responds quickly. Building on these shortcomings, we propose a deep reinforcement learning approach that makes time-aware decisions to trade stocks while optimizing profit using textual data. Our method outperforms state-of-the-art in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets (English) and financial news (Chinese) pertaining to two major indexes and four global stock markets. Through extensive experiments and studies, we build the case for our method as a tool for quantitative trading.",
    }

================
File: configs_stock.py
================
# shares normalization factor
# 100 shares per trade
HMAX_NORMALIZE = 100
# initial amount of money we have in our account
INITIAL_ACCOUNT_BALANCE = 100000
# total number of stocks in our portfolio
STOCK_DIM = 88
N_DAYS = 7
MAX_TWEETS = 30
TWEETS_EMB = 768
TIME_FEATS = STOCK_DIM * N_DAYS * MAX_TWEETS
# Shape:  [Current Balance] + [Last Day Prices] + [Target Day Prices] + [Owned Shares] + [Text Features + Tweet Length]
FEAT_DIMS = (
    1
    + STOCK_DIM * 3
    + STOCK_DIM * N_DAYS * MAX_TWEETS * TWEETS_EMB
    + STOCK_DIM * N_DAYS
    + 5 * STOCK_DIM
    + TIME_FEATS
)

# all starting indexes
# [INITIAL_ACCOUNT_BALANCE]  # balance
# + last_price  # stock prices initial
# + [0] * STOCK_DIM  # stocks on hold
# + emb_data  # tweet features
# + len_data  # tweet len
# + target_price  # target price
# + price_diff
# + vol_diff
# + text_diff
# + price_text_diff
# + all_diff
# + time_feats

LAST_PRICE_IDX = 1
HOLDING_IDX = LAST_PRICE_IDX + STOCK_DIM
EMB_IDX = HOLDING_IDX + STOCK_DIM
LEN_IDX = EMB_IDX + STOCK_DIM * N_DAYS * MAX_TWEETS * TWEETS_EMB
TARGET_IDX = LEN_IDX + STOCK_DIM * N_DAYS
PRICEDIFF_IDX = TARGET_IDX + STOCK_DIM
VOLDIFF_IDX = PRICEDIFF_IDX + STOCK_DIM
TEXTDIFF_IDX = VOLDIFF_IDX + STOCK_DIM
PRICE_TEXT_DIFF_IDX = TEXTDIFF_IDX + STOCK_DIM
ALLDIFF_IDX = PRICE_TEXT_DIFF_IDX + STOCK_DIM
TIME_IDX = ALLDIFF_IDX + STOCK_DIM
TRANSACTION_FEE_PERCENT = 0.1
REWARD_SCALING = 1e-4

================
File: ddpg.py
================
import numpy as np
import torch
import torch.nn as nn
from torch.optim import Adam
from memory import SequentialMemory
from random_process import OrnsteinUhlenbeckProcess
from util import *

criterion = nn.MSELoss()

class DDPG(object):
    def __init__(self, nb_states, nb_actions, args):

        if args.seed > 0:
            self.seed(args.seed)
        if args.model == "profit": 
            from model import Actor, Critic
        self.nb_states = nb_states
        self.nb_actions = nb_actions

        # Create Actor and Critic Network
        self.actor = Actor()
        self.actor_target = Actor()
        self.actor_optim = Adam(self.actor.parameters(), lr=args.prate)

        self.critic = Critic()
        self.critic_target = Critic()
        self.critic_optim = Adam(self.critic.parameters(), lr=args.rate)

        hard_update(
            self.actor_target, self.actor
        )  # Make sure target is with the same weight
        hard_update(self.critic_target, self.critic)

        # Create replay buffer
        self.memory = SequentialMemory(
            limit=args.rmsize, window_length=args.window_length
        )
        self.random_process = OrnsteinUhlenbeckProcess(
            size=nb_actions, theta=args.ou_theta, mu=args.ou_mu, sigma=args.ou_sigma
        )

        # Hyper-parameters
        self.batch_size = args.bsize
        self.tau = args.tau
        self.discount = args.discount
        self.depsilon = 1.0 / args.epsilon

        #
        self.epsilon = 1.0
        self.s_t = None  # Most recent state
        self.a_t = None  # Most recent action
        self.is_training = True

        #
        if USE_CUDA:
            self.cuda()

    def update_policy(self):
        # Sample batch
        (
            state_batch,
            action_batch,
            reward_batch,
            next_state_batch,
            terminal_batch,
        ) = self.memory.sample_and_split(self.batch_size)

        # Prepare for the target q batch
        next_q_values = self.critic_target(
            to_tensor(next_state_batch, volatile=True),
            self.actor_target(to_tensor(next_state_batch, volatile=True)),
        )
        next_q_values.volatile = False

        target_q_batch = (
            to_tensor(reward_batch)
            + self.discount * to_tensor(terminal_batch.astype(np.float)) * next_q_values
        )

        # Critic update
        self.critic.zero_grad()

        q_batch = self.critic(to_tensor(state_batch), to_tensor(action_batch))

        value_loss = criterion(q_batch, target_q_batch)
        value_loss.backward()
        self.critic_optim.step()

        # Actor update
        self.actor.zero_grad()

        policy_loss = -self.critic(
            to_tensor(state_batch), self.actor(to_tensor(state_batch))
        )

        policy_loss = policy_loss.mean()
        policy_loss.backward()
        self.actor_optim.step()

        # Target update
        soft_update(self.actor_target, self.actor, self.tau)
        soft_update(self.critic_target, self.critic, self.tau)

    def eval(self):
        self.actor.eval()
        self.actor_target.eval()
        self.critic.eval()
        self.critic_target.eval()

    def train(self):
        self.actor.train()
        self.actor_target.train()
        self.critic.train()
        self.critic_target.train()

    def cuda(self):
        self.actor.cuda()
        self.actor_target.cuda()
        self.critic.cuda()
        self.critic_target.cuda()

    def observe(self, r_t, s_t1, done):
        if self.is_training:
            self.memory.append(self.s_t, self.a_t, r_t, done)
            self.s_t = s_t1

    def random_action(self):
        action = np.random.uniform(-1.0, 1.0, self.nb_actions)
        self.a_t = action
        return action

    def select_action(self, s_t, decay_epsilon=True):
        action = to_numpy(self.actor(to_tensor(np.array([s_t])))).squeeze(0)
        action += self.is_training * max(self.epsilon, 0) * self.random_process.sample()
        action = np.clip(action, -1.0, 1.0)

        if decay_epsilon:
            self.epsilon -= self.depsilon

        self.a_t = action
        return action

    def reset(self, obs):
        self.s_t = obs
        self.random_process.reset_states()

    def load_weights(self, output):
        if output is None:
            return

        self.actor.load_state_dict(torch.load("{}/actor.pkl".format(output)))
        self.actor_target.load_state_dict(
            torch.load("{}/actor_target.pkl".format(output))
        )
        self.critic.load_state_dict(torch.load("{}/critic.pkl".format(output)))
        self.critic_target.load_state_dict(
            torch.load("{}/critic_target.pkl".format(output))
        )

    def save_model(self, output):
        torch.save(self.actor.state_dict(), "{}/actor.pkl".format(output))
        torch.save(self.critic.state_dict(), "{}/critic.pkl".format(output))
        torch.save(self.actor_target.state_dict(), "{}/actor_target.pkl".format(output))
        torch.save(
            self.critic_target.state_dict(), "{}/critic_target.pkl".format(output)
        )

    def seed(self, s):
        torch.manual_seed(s)
        if USE_CUDA:
            torch.cuda.manual_seed(s)

================
File: main.py
================
import numpy as np
import argparse
from copy import deepcopy
import torch
import gym
import pickle
from env import StockEnvTrade
from evaluator import Evaluator
from ddpg import DDPG
from util import *
from tqdm import tqdm
import os

def train(
    num_iterations,
    agent,
    env_train,
    env_test,
    evaluate,
    validate_steps,
    output,
    max_episode_length=None,
    debug=False,
):

    agent.is_training = True
    step = episode = episode_steps = 0
    episode_reward = 0.0
    best_sharpe = -99999
    best_reward = -99999
    best_sortino = -99999
    best_calmar = -99999
    best_mdd = -99999
    best_cum_returns = -99999

    observation = None
    pbar = tqdm(total=num_iterations)
    while step < num_iterations:

        # reset if it is the start of episode
        if observation is None:
            observation = deepcopy(env_train.reset())
            agent.reset(observation)

        # agent pick action ...
        if step <= args.warmup:
            action = agent.random_action()
        else:
            action = agent.select_action(observation)

        # env response with next_observation, reward, terminate_info
        observation2, reward, done, info = env_train.step(action)
        observation2 = deepcopy(observation2)
        if max_episode_length and episode_steps >= max_episode_length - 1:
            done = True

        # agent observe and update policy
        agent.observe(reward, observation2, done)
        if step > args.warmup:
            agent.update_policy()

        # [optional] evaluate
        if evaluate is not None and validate_steps > 0 and step % validate_steps == 0:
            print("VALIDATING!!! step = ", step)
            agent.is_training = False
            agent.eval()
            policy = lambda x: agent.select_action(x, decay_epsilon=False)

            (
                validate_reward,
                validate_sharpe,
                validate_sortino,
                validate_calmar,
                validate_mdd,
                validate_cum_returns,
            ) = evaluate(env_test, policy, debug=debug, visualize=True, save=False)

            agent.train()
            agent.is_training = True

            print(
                "[Evaluate] Step_{:07d}: mean_reward:{} mean_sharpe:{} mean_sortino:{} mean_calmar:{} mean_mdd:{} mean_cum_returns:{}".format(
                    step,
                    validate_reward,
                    validate_sharpe,
                    validate_sortino,
                    validate_calmar,
                    validate_mdd,
                    validate_cum_returns,
                )
            )
            if validate_sharpe > best_sharpe:
                print("saving model!!!!")
                best_sharpe = validate_sharpe
                best_reward = validate_reward
                best_sortino = validate_sortino
                best_calmar = validate_calmar
                best_mdd = validate_mdd
                best_cum_returns = validate_cum_returns
                agent.save_model(output)
                if not os.path.exists(os.path.join(output, "validate_reward")):
                    os.makedirs(os.path.join(output, "validate_reward"))
                evaluate.save_results(os.path.join(output, "validate_reward"))

            print(
                "[BEST] Step_{:07d}: mean_reward:{} mean_sharpe:{} mean_sortino:{} mean_calmar:{} mean_mdd:{} mean_cum_returns:{}".format(
                    step,
                    best_reward,
                    best_sharpe,
                    best_sortino,
                    best_calmar,
                    best_mdd,
                    best_cum_returns,
                )
            )
            print("output:", output)

        # update
        step += 1
        episode_steps += 1
        episode_reward += reward
        observation = deepcopy(observation2)

        if done:  # end of episode
            if debug:
                prGreen(
                    "#{}: episode_reward:{} steps:{}".format(
                        episode, episode_reward, step
                    )
                )

            agent.memory.append(
                observation, agent.select_action(observation), 0.0, False
            )

            # reset
            observation = None
            episode_steps = 0
            episode_reward = 0.0
            episode += 1
        pbar.update(1)

def test(num_episodes, agent, env, evaluate, model_path, visualize=True, debug=False):

    agent.load_weights(model_path)
    agent.is_training = False
    agent.eval()
    policy = lambda x: agent.select_action(x, decay_epsilon=False)

    for i in range(num_episodes):
        validate_reward = evaluate(
            env, policy, debug=debug, visualize=visualize, save=False
        )
        if debug:
            prYellow("[Evaluate] #{}: mean_reward:{}".format(i, validate_reward))

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="PyTorch on TORCS with Multi-modal")

    parser.add_argument(
        "--mode", default="train", type=str, help="support option: train/test"
    )
    parser.add_argument(
        "--env", default="Humanoid-v2", type=str, help="open-ai gym environment"
    )
    parser.add_argument(
        "--hidden1",
        default=400,
        type=int,
        help="hidden num of first fully connect layer",
    )
    parser.add_argument(
        "--hidden2",
        default=300,
        type=int,
        help="hidden num of second fully connect layer",
    )
    parser.add_argument("--rate", default=0.001, type=float, help="learning rate")
    parser.add_argument(
        "--prate",
        default=0.0001,
        type=float,
        help="policy net learning rate (only for DDPG)",
    )
    parser.add_argument(
        "--warmup",
        default=25,
        type=int,
        help="time without training but only filling the replay memory",
    )
    parser.add_argument("--discount", default=0.99, type=float, help="")
    parser.add_argument("--bsize", default=1, type=int, help="minibatch size")
    parser.add_argument("--rmsize", default=25, type=int, help="memory size")
    parser.add_argument("--window_length", default=1, type=int, help="")
    parser.add_argument(
        "--tau", default=0.001, type=float, help="moving average for target network"
    )
    parser.add_argument("--ou_theta", default=0.15, type=float, help="noise theta")
    parser.add_argument("--ou_sigma", default=0.2, type=float, help="noise sigma")
    parser.add_argument("--ou_mu", default=0.0, type=float, help="noise mu")
    parser.add_argument(
        "--validate_episodes",
        default=1,
        type=int,
        help="how many episode to perform during validate experiment",
    )
    parser.add_argument("--max_episode_length", default=500, type=int, help="")
    parser.add_argument(
        "--validate_steps",
        default=25,
        type=int,
        help="how many steps to perform a validate experiment",
    )
    parser.add_argument("--output", default="output", type=str, help="")
    parser.add_argument("--debug", dest="debug", action="store_true")
    parser.add_argument("--init_w", default=0.003, type=float, help="")
    parser.add_argument(
        "--train_iter", default=200000, type=int, help="train iters each timestep"
    )
    parser.add_argument(
        "--epsilon", default=50000, type=int, help="linear decay of exploration policy"
    )
    parser.add_argument("--seed", default=-1, type=int, help="")
    parser.add_argument(
        "--resume", default="default", type=str, help="Resuming model path for testing"
    )
    parser.add_argument(
        "--model", default="time", type=str, help="which model to choose <profit, simple, plain>"
    )
    parser.add_argument(
        "--diff",
        default=None,
        type=str,
        help="diff for reward <price, vol, text, price_text, pvt> default: (None)",
    )

    args = parser.parse_args()
    output = get_output_folder(args.output, args.env)
    if args.resume == "default":
        resume = "output/{}-run0".format(args.env)

    with open("../data/traindata_ussnp500_rl.pkl", "rb") as f:
        data_train = pickle.load(f)

    with open("../data/testdata_ussnp500_rl.pkl", "rb") as f:
        data_test = pickle.load(f)

    env_train = StockEnvTrade(data_train, 0, args)
    env_test = StockEnvTrade(data_test, 0, args)

    nb_states = env_train.observation_space.shape[0]
    nb_actions = env_train.action_space.shape[0]

    agent = DDPG(nb_states, nb_actions, args)
    evaluate = Evaluator(
        args.validate_episodes,
        args.validate_steps,
        output,
        max_episode_length=len(data_test),
    )

    if args.mode == "train":
        train(
            args.train_iter,
            agent,
            env_train,
            env_test,
            evaluate,
            args.validate_steps,
            output,
            max_episode_length=len(data_train),
            debug=args.debug,
        )

    elif args.mode == "test":
        test(
            args.validate_episodes,
            agent,
            env_test,
            evaluate,
            resume,
            visualize=True,
            debug=args.debug,
        )

    else:
        raise RuntimeError("undefined mode {}".format(args.mode))

================
File: evaluator.py
================
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import savemat
import pickle as pkl
from util import *

class Evaluator(object):
    def __init__(self, num_episodes, interval, save_path="", max_episode_length=None):
        self.num_episodes = num_episodes
        self.max_episode_length = max_episode_length
        self.interval = interval
        self.save_path = save_path
        self.results = np.array([]).reshape(num_episodes, 0)
        self.rewards_list = []
        self.asset_memory = []

    def __call__(self, env, policy, debug=False, visualize=False, save=True):
        self.is_training = False
        observation = None
        result = []
        sharpe = []
        sortino = []
        calmar = []
        mdd = []
        cum_returns = []
        rewards_list = []

        for episode in range(self.num_episodes):

            # reset at the start of episode
            observation = env.reset()
            episode_steps = 0
            episode_reward = 0.0

            assert observation is not None

            # start episode
            done = False
            while not done:
                action = policy(observation)

                observation, reward, done, info = env.step(action)
                if (
                    self.max_episode_length
                    and episode_steps >= self.max_episode_length - 1
                ):
                    done = True

                if visualize:
                    env.render(mode="human")

                # update
                episode_reward += reward
                rewards_list.append(reward)
                episode_steps += 1

            if debug:
                prYellow(
                    "[Evaluate] #Episode{}: episode_reward:{}".format(
                        episode, episode_reward
                    )
                )

            result.append(episode_reward)
            sortino.append(info["sortino"])
            calmar.append(info["calmar"])
            sharpe.append(info["sharpe"])
            mdd.append(info["mdd"])
            cum_returns.append(info["cum_returns"])

        self.asset_memory = env.asset_memory

        result = np.array(result).reshape(-1, 1)
        self.results = np.hstack([self.results, result])
        self.rewards_list = rewards_list

        return (
            np.mean(result),
            np.mean(sharpe),
            np.mean(sortino),
            np.mean(calmar),
            np.mean(mdd),
            np.mean(cum_returns),
        )

    def save_results(self, fn):
        fig, ax = plt.subplots(1, 1, figsize=(6, 5))
        plt.xlabel("Timestep")
        plt.ylabel("Asset_Value")
        plt.plot(self.asset_memory, "r")
        plt.savefig(os.path.join(fn, "asset_value.png"))
        with open(os.path.join(fn, "asset_value.pkl"), "wb") as f:
            pkl.dump(self.asset_memory, f)
        plt.close()
